# config.yaml

# Objective 2: MDP and Simulation Environment Configuration

env:
  action_type: discrete # Options: discrete, continuous
  discrete_action_map: [0.9, 0.95, 1.0, 1.05, 1.1] # Price multipliers for discrete actions
  action_low: 0.9 # Lower bound for continuous action price multiplier
  action_high: 1.1 # Upper bound for continuous action price multiplier
  reward_formulation: revenue # Options: revenue, profit, revenue_minus_volatility
  demand_simulator_approach: parametric # Options: parametric, ml_model
  episode_horizon: 365 # Number of days in one episode

  # Parametric Demand Simulator Parameters
  parametric_simulator:
    beta_price: -2.0 # Price elasticity coefficient
    noise_std: 0.0719 # Standard deviation of demand noise
    base_demand: 100.0 # Base demand when price is reference price
    ref_price: 1.0 # Reference price for elasticity calculation

  # ML Model Demand Simulator Parameters
  ml_model_simulator:
    noise_std: 0.0719 # Standard deviation of demand noise calibrated from residuals

# Objective 3: DRL Agent Training Configuration

training:
  agent: "DQN" # Default agent, will be overridden by experiment files
  total_timesteps: 100000
  eval_freq: 10000
  n_eval_episodes: 10
  seed: 42
  product_id: "PRD0904358" # Product to train on

# Cross-cutting Configuration
paths:
  raw_data: data/transactions.parquet
  processed_data_dir: data/processed
  scalers_dir: models/scalers
  models_dir: models
  test_data: data/processed/test_scaled.parquet
  price_scaler: models/scalers/avg_price_standardscaler.joblib
  features_scaler: models/scalers/features_standardscaler.joblib # Assuming this exists
  top_100_ids: data/processed/top100_ids.json