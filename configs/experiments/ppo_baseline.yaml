# configs/experiments/ppo_baseline.yaml
agent: ppo
training:
  n_envs: 4
  agent: "PPO" # Explicitly set agent for training to PPO
agent_params:
  ppo:
    embedding_dim: 16
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 2048
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.05
    policy_kwargs:
      net_arch: [256, 256] # Two hidden layers with 256 units each
