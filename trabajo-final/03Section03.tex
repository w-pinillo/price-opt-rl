\chapter{Materials and Methods}

% TEMPORARY NOTE: Summary of the data preparation pipeline (Objective 1).
% This section will be expanded and integrated into the main text later.

\subsection*{Data Preparation Pipeline}

The initial phase of the project involved transforming raw transactional data into a structured, feature-rich dataset suitable for training reinforcement learning agents. This process was executed through a sequential pipeline, ensuring reproducibility and modularity. The key steps are summarized below:

\begin{itemize}
    \item \textbf{Consolidation and Filtering:} The process began by consolidating all raw transaction files into a single dataset. From this, the top 100 products, ranked by total units sold, were selected for the study. The data was then aggregated to a daily level for each product, creating a consistent time-series format.

    \item \textbf{Feature Engineering:} To provide the agent with a comprehensive view of the market dynamics, a rich set of features was engineered:
    \begin{itemize}
        \item \textbf{Demand Lags:} Lagged demand features for 1, 7, 14, and 28 days were created to capture short-term and weekly demand patterns.
        \item \textbf{Rolling Statistics:} To model recent trends and volatility, 7-day and 28-day rolling means and standard deviations of demand were calculated.
        \item \textbf{Seasonality:} Cyclical features using sine and cosine transformations were generated for both the day of the week and the month of the year to effectively model periodic patterns in demand.
    \end{itemize}

    \item \textbf{Data Splitting and Scaling:} The dataset was split chronologically into training, validation, and test sets to ensure that the model is evaluated on future, unseen data, preventing any lookahead bias. All numeric features in the training set were standardized using a \texttt{StandardScaler}, which was then saved and applied consistently across the validation and test sets.
\end{itemize}

The output of this pipeline was a set of processed, scaled datasets (`train`, `val`, `test`) and the corresponding feature scaler, providing a solid foundation for the subsequent stages of model training and evaluation.

% TEMPORARY NOTE: Explanation of DQNPolicy
% This section will be expanded later.

\subsection*{Technical Note: DQNPolicy Implementation in Stable-Baselines3}

The \texttt{DQNPolicy} class is central to the agent's learning process. It manages the neural networks that approximate the Q-function, which estimates the value of taking an action in a given state. Its operation can be summarized in a few key points:

\begin{itemize}
    \item \textbf{Two-Network Architecture:} The policy initializes two identical neural networks:
    \begin{itemize}
        \item \textbf{Online Network (\texttt{q\_net}):} This network is used for action selection during interaction with the environment. Its weights are continuously updated during training.
        \item \textbf{Target Network (\texttt{q\_net\_target}):} This is a periodically updated, frozen copy of the online network. It is used to provide a stable target for the learning updates, which is a crucial technique to prevent oscillations and stabilize the training process.
    \end{itemize}

    \item \textbf{Action Selection:} To choose an action, the current state is fed into the online network, which outputs a Q-value for every possible action. The policy then selects the action with the highest Q-value (a greedy policy).

    \item \textbf{Learning Process:} The learning occurs by sampling a batch of experiences from a replay buffer. The online network's weights are updated to minimize the loss between its predicted Q-values and the target Q-values calculated using the target network. Specifically, the target is computed as:
    \[ y = r + \gamma \max_{a'} Q_{\text{target}}(s', a') \]
    The loss is then typically the Mean Squared Error between this target $y$ and the prediction from the online network, $Q_{\text{online}}(s, a)$.
\end{itemize}

% TEMPORARY NOTE: Explanation of PPO
% This section will be expanded later.

\subsection*{Technical Note: PPO Implementation in Stable-Baselines3}

Proximal Policy Optimization (PPO) is an \textbf{Actor-Critic} algorithm that has become a go-to choice for many reinforcement learning problems due to its reliable performance and ease of implementation. Unlike value-based methods like DQN, PPO learns a policy directly.

Its operation can be summarized as follows:

\begin{itemize}
    \item \textbf{Actor-Critic Architecture:} PPO uses two distinct neural networks:
    \begin{itemize}
        \item \textbf{The Actor:} This network represents the policy. It takes the current state as input and outputs a probability distribution over the possible actions. It is responsible for deciding which action to take.
        \item \textbf{The Critic:} This network estimates the state-value function, $V(s)$. Its job is not to choose actions, but to evaluate how good a state is, which in turn helps the Actor learn more efficiently by providing a low-variance estimate of performance.
    \end{itemize}

    \item \textbf{The PPO Objective Function:} The core innovation of PPO is its objective function, which prevents the policy from changing too drastically at each update step. This is achieved with a "clipped surrogate objective".
    \begin{enumerate}
        \item It calculates the probability ratio between the new policy and the old policy: $r_t = \pi_{\text{new}}(a|s) / \pi_{\text{old}}(a|s)$.
        \item It uses the Critic to compute an \textbf{Advantage} value ($\hat{A}_t$), which represents how much better an action was than the average action in that state.
        \item The policy update is then "clipped" to keep the new policy close to the old one, avoiding destructive updates. The simplified objective is:
        \[ L_{\text{CLIP}} = \mathbb{E} \left[ \min \left( r_t \hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right] \]
    \end{enumerate}

    \item \textbf{Learning Process:} PPO's training loop is different from DQN's.
    \begin{itemize}
        \item The Actor interacts with the environment for a set number of steps to collect a batch of experiences.
        \item Using this batch, the Critic helps compute the advantage for each step.
        \item The algorithm then performs multiple epochs of gradient ascent on the collected data, updating both the Actor (using the clipped objective) and the Critic (by minimizing the error in its value predictions). This ability to reuse experience for multiple updates makes it more data-efficient than simpler policy gradient methods.
    \end{itemize}
\end{itemize}